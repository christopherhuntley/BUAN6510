{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L10_Dimensional_DW.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"11MBPducEj0HgT0u-nrCE60AvHJQnPR3y","authorship_tag":"ABX9TyMhk50+lf3Zr9DTFNJamtNl"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5rHzwUorF5ab"},"source":["<img src=\"https://github.com/christopherhuntley/BUAN6510/blob/master/img/Dolan.png?raw=true\" width=\"180px\" align=\"right\">\n","\n","# **BUAN 6510**\n","# **Lesson 10: Dimensional Data Warehouse Design** \n","_Data Driven Analytics_\n","\n","## **Learning Objectives**\n","### **Theory / Be able to explain ...**\n","- How database design for analytical applications is different from that for transaction systems\n","- The various forms of the star schema and when each is most applicable\n","- The concept of data granularity in dimensional data warehouses\n","- Data cubes as a dimensional data model\n","- Recent SQL extensions geared towards analytical applications \n","\n","### **Skills / Know how to ...**\n","- Identify dimensions that provide context to facts\n","- Use SQL window functions to work with longitudinal data\n","- Use SQL arrays and structs to work with repeating groups and hierarchical data structures\n","\n","--------\n","## **LESSON 10 HIGHLIGHTS**"]},{"cell_type":"code","metadata":{"id":"I95WVwkCnTJF","colab":{"base_uri":"https://localhost:8080/","height":580},"cellView":"form","executionInfo":{"status":"ok","timestamp":1614963475950,"user_tz":300,"elapsed":268,"user":{"displayName":"Christopher Huntley","photoUrl":"","userId":"11069151036176747256"}},"outputId":"df3272a3-b49d-4c5a-ddc0-9e20a821d7fb"},"source":["#@title Run this cell if video does not appear\n","%%html\n","<div style=\"max-width:1000px\">\n","  <div style=\"position: relative;padding-bottom: 56.25%;height: 0;\">\n","    <iframe style=\"position: absolute;top: 0;left: 0;width: 100%;height: 100%;\" rel=\"0\" modestbranding=\"1\"  src=\"https://www.youtube.com/embed/ZSVRiOfodDY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","  </div>\n","</div>"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"max-width:1000px\">\n","  <div style=\"position: relative;padding-bottom: 56.25%;height: 0;\">\n","    <iframe style=\"position: absolute;top: 0;left: 0;width: 100%;height: 100%;\" rel=\"0\" modestbranding=\"1\"  src=\"https://www.youtube.com/embed/ZSVRiOfodDY\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","  </div>\n","</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"euT4WJUOymsJ"},"source":["## **BIG PICTURE: The Holy Grail of Data Warehousing**\n","\n","Data Warehousing as a universal data repository has been a dream since Ralph Kimball and Bill Inmon popularized the term in the 1980s. Analysts had by that point built up quite a repertoire of models for just about any kind of analysis. They could create classification and regression trees (decision trees, random forests, etc.). They could do linear, nonlinear, kernel, and logistic regression with large datasets. They could solve optimization problems with thousands of variables and tens of thousands of constraints. Even the neural network models at the core of the latest and greatest deep learning techniques were pretty mature by 1994.  \n","\n","What was missing was data! Well, sort of. What we had was lots of transactional data locked up in siloed mainframe systems. Online banking (via ATMs) had been a reality since the 1970s, credit card transactions since the 1960s, and the air traffic control system since the 1950s. These systems were great at capturing event data, one event at a time. They could even handle bulk transactions and reporting, if you could wait for the job to run overnight. And heaven forbid that you might want data in a different format or that wasn't already included in a standard report.  \n","\n","However, analytical data is not the same as transactional data. Among the differences: \n","- **Temporal Scope:** Emphasis on historical data rather than real-time operations\n","- **Diversity:** Drawing on multiple sources instead of a single transaction system\n","- **Data Quality:** \"good enough to run the company\" was not good enough for analytical modeling\n","- **Performance Tradeoffs:** Focus on computational speed (on desktop computers) instead of data throughput (on shared mainframes)\n","- **Access:** Providing analysts with direct access to transaction systems is both insecure and expensive\n","\n","Even back then the ultimate solutions were known, though not remotely close to being available. Consider, for example, this figure from Kimball's seminal book *The Data Warehouse Lifecycle Toolkit*, published in 1998 and based on original work **from the mid-1980s**.\n","\n","![Kimball's Data Warehouse](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L10_Kimball_Data_Warehouse_Elements.png)\n","\n","All of the elements of a modern data pipeline are there. It even articulated the steps of the ETL process in detail. It was all there ... to be realized *someday*. \n","\n","Someday is now. With commodity data storage, ample computing power, ready-made software for just about any kind of modeling, and the analytical results to attract attention from management, data infrastructure is finally seen as what it should have been all along: a critical resource upon which the company relies to make it stand out against the competition. \n","\n","That's what the vendors tell us, anyway. \n","\n","In this lesson we will explore the **Dimensional Data Warehouse** model first proposed by Kimball et al. all those years ago. We will also consider how its mass adoption has influenced the SQL standard in recent years, with the addition of extensions like  **arrays**, **structs**, and  **partitions** that relax fundamental assumptions of the relational data model. "]},{"cell_type":"markdown","metadata":{"id":"LCSbx4saW1Hu"},"source":["---\n","## **The Star Schema Pattern**\n","\n","The star schema used by dimensional data warehouses is what we call a **design pattern**, a standard solution to a standard problem. By giving each pattern a name, designers can discuss their work with others without having to explain their decisions anew each time. As design patterns pass into common usage, they form a kind of shorthand design language that works in a given problem domain. \n","\n","The star schema pattern addresses the disconnect between the way data is recorded versus how it is used by analysts. It reduces all data down to measures (facts) and labels (dimensions). Since *all information* takes that form eventually anyway, the pattern strikes a nice balance between structure and general applicability. \n","\n","In our dimensional model, we end up with several moderately-sized **dimension tables** surrounding a large (possibly massive) **fact table**:\n","- The dimensions are somewhat timeless and immutable. They define the **context** for the facts. Even when the facts themselves may change over time, the dimensions remain relatively static. \n","- The facts are somewhat volatile, with new **measures** continually added and others redefined to suit the ever changing needs of the analysts. If there is a way to precompute a statistic or other measure so analysts don't have to, then do it. If a given measure is no longer needed or misleading, then redefine or remove it. \n","\n","Once again, here is the NBA PlayFacts warehouse, this time noting some of the key features. We will use is as an example,starting with the dimensions before moving on to the facts. We will also explore variations on the general star schema pattern that fit certain use cases. \n","![NBA PlayFacts Dim DW](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L10_Star_Schema_Notes.png)"]},{"cell_type":"markdown","metadata":{"id":"oHB_1psdXByM"},"source":["### **Dimension Tables**\n","Dimensions represent the contextual language that we use to describe a given fact. In other words, the dimensions define the lens through which we view each fact. They are what give it context and meaning. \n","\n","In theory the **dimensions exist before the fact data is collected**. So, how do we know what they are before we have any data? A good starting point is the [Five Ws framework](https://en.wikipedia.org/wiki/Five_Ws) used by journalists and storytellers the world over:\n","- **Who was involved?** People, roles, etc. \n","- **What happened** Event types, outcomes, etc. \n","- **When did it happen?** Timing or place in a sequence\n","- **Where did it happen?** Location, which may be conceptual rather than physical\n","- **Why did it happen?** Intent, cause, etc. \n","- **How did it happen?** Steps, sequential logic, etc. \n","\n","Though there is some disagreement about this, the usual recommendation is that dimension tables be fully denormalized. Since they are often fairly small (relative to the facts table) and don't change much, there is little chance of creating anomalies over time. So, while it may be tempting to, for example, normalize out zip codes and cities from a location dimension, there is no real need, especially when it would require an unnecessary table join.\n","\n","As we can see in the NBA example, the first question $-$ who is involved in a given play $-$ is answered with three dimensions: \n","- the individual player who gets credited with each event\n","- the lineup of players on the court at the time\n","- the team whose play is being reflected by the facts\n","\n","It is done this way to support different, independently-calculated measures:\n","- the counting stats (points, rebounds, assists, etc.) for an individual player, lineup, or team\n","- the total playing time for each player or lineup  on the court, even when they are not generating counting stats\n","\n","An often overlooked but potentially tricky aspect of dimension design is whether dimensions are allowed to overlap. In other words, can the same dimension be represented two different ways? Can we combine dimensions to create a third uber-dimension? \n","\n","Like a lot of things, it depends. For example, we could combine geo-location data (addresses) with organizational hierarchy (offices, districts, regions, etc.) into a single dimension if that is how the data is usually grouped. The result is several levels of **granularity**, all stored in one dimension table. For a different analytical application, however, we may want to keep things more separated, especially if the geographic locations overlap. It may be that, for example, the same physical location may mix personnel from multiple functions or divisions within a company. Then it would not make sense to treat locations as nodes in the organizational hierarchy. \n","\n","We will go into this idea more in the Pro Tips section, where we will discuss the peculiar logic of longitudinal segmentation (i.e., slicing time)."]},{"cell_type":"markdown","metadata":{"id":"NekcmKiCsxQm"},"source":["\n","\n"," \n","\n","### **Fact Tables**\n","Fact tables exist at the intersection of the dimension tables. Each fact is labeled with foreign keys to the dimension tables, usually one key per dimension. The rest of the columns are measures that can be used in aggregate calculations.  \n","\n","What makes a good measure? Anything from which we can calculate a descriptive statistic: \n","- For text data, we generally are limited to the text itself and counts of some sort. We may, for example, count the number of times the word \"no\" appears, how many sentences there are, etc. \n","- For numerical data we can use all of the usual statistics like mean, maximum, minimum, etc. \n","- For temporal data (dates and times), we may calculate elapsed times, inter-event times, cumulative times, etc. that can be treated like numerical data\n","- For binary data (pictures, etc.), the options are very limited, though one may be able to apply a machine learning technique to generate numerical digests that can be aggregated.  \n","\n","Interestingly, the measures can only be as granular as the dimensions allow. In other words, if a given dimension only has 3 possible labels, then that dimension can only divide up the facts three ways. Each additional dimension, however, increases the granularity accordingly. If every dimension were to have two possible values, then a one single dimension could divide the facts into 2 groups. With two dimensions, we could divide into up to 4 groups. With three dimensions, we could generate up to 8 groups, etc. \n","\n","One way to visualize this is with a (hyper-) cube, with each dimension on a side. Each fact is *binned* inside one of the smaller cubes at the intersection of the dimensions. For the NBA PlayFacts cube below, each fact is binned based on the game, team, and player. Thus, if with only three dimensions, we would only be able to generate box score stats for full games. In order to get statistics within a game (e.g., for the last two minutes of each period) we would need to include the play segment dimension. (Don't ask about how we'd show a 4-dimensional cube. Just know that we can.)\n","\n","![Data Cube](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L10_DataCube_wide.png)\n","\n","> **Heads Up:** It is sometimes difficult to distinguish dimensions from measures when source data is numerical. For example, is the time on the clock (i.e., seconds remaining in the period) a measure or a dimension? It is a measure, in the sense that it captures the passage of time, but it is also a dimension, in that it records when a given event happened. The key when examining any given quantity is to ask whether you would ever aggregate it (sum, average, etc.) or just cite it. In a basketball game it is the latter, so we separate it out into the play segment dimension. The clock *interval* between events (elapsed time), however, is something that we can sum up by quarter, player, etc. Thus, it belongs on the fact table.  \n"," \n"]},{"cell_type":"markdown","metadata":{"id":"TLgrt7SbXMvV"},"source":["### **Do we really need dimension *tables*?**\n","One of the advantages of keeping dimensional data in separate tables is that it can significantly reduce storage costs by eliminating redundant data. However, with the advent of cloud-based data storage at commodity prices, storage becomes less important than performance. Thus, we may choose to denormalize everything into a single table that doesn't require any expensive joins. It's simple enough. If we already have the data in a normalized form, then we would just need to join in every table and select every column to generate a new \"one table fits all\" data warehouse. The dimensions would still be there, just as columns instead of tables.\n","\n","While that sounds great in theory, in practice there are two good reasons for creating separate dimension tables: \n","- **If care is taken to *use just foreign keys in the `GROUP BY` clause*, then it can actually be faster to query multiple tables than a single table.** This is because the joins will happen **after** the grouping has reduced the data to a manageable number of fact table rows. The incremental performance cost of the join is then practically nil, especially if there are a small number of groups in the result set. \n","- **Dimension tables provide opportunities to add in static descriptive data.** For example, we could add in the seating capacity or age of a given basketball arena if we treat it as a dimension table instead of just a column. \n","\n","Whether either of these advantages are relevant depends on the situation. As a general rule, unless you have a good reason not to, it is best to create dimension tables instead of dimension columns. "]},{"cell_type":"markdown","metadata":{"id":"3bz9O8d7XSev"},"source":["### **Snowflakes and Galaxies**\n","\n","The rule that dimensions be denormalized is more of a convenience than a law. The purpose is to make writing queries as simple and bug-free as possible. However, if we are ultimately going to denormalize the data using the same joins every time, then what difference does it make how many joins there are? Such is the reasoning behind the so-called **Snowflake design pattern**. \n","\n","Here, for example, is the snowflake version of the PlayFacts database. The dotted lines show relationships normalized out from the star schema dimensions. There is no effect on the fact table, just more detail in the dimensions. \n","![](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L10_NBA_PlayFacts_Snowflake_DW.png) \n","\n","The advantages of normalizing the dimensions include:\n","- less data redundancy, smaller storage requirements, etc. that we usually associate with normalization\n","- the ability to add new details (like team franchises or arenas) without disrupting existing dimensions\n","\n","It is impotant to point out that the snowflake and star schema patterns are both perfectly valid. Which to use is totally situational. Let the data, its usage, and performance considerations guide your decision. \n","\n","Like the snowflake pattern, the **galaxy pattern** also extends the star schema, this time by allowing multiple fact tables. The baseball database from lesson 2 is a galaxy with ...\n","- Fact tables for fielding, batting, pitching, etc.  \n","- Dimension tables for players (Master), teams, all star game appearances, etc. \n","It also follows the snowflake pattern, with enhancements like All Star Game appearances, Hall of Fame voting, etc. attached to the players and teams. \n","\n","![Lahman 2016 ERD](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L2_baseball_stats_schema.png)\n"]},{"cell_type":"markdown","metadata":{"id":"yUNZeFcYCU9A"},"source":["---\n","## **Rollups and Drilldowns**\n","\n","One of the big advantages of a dimensional data warehouse design is that it make it very simple to aggregate and disaggregate data at various levels of granularity. \n","\n","> **Heads Up**: The terms \"rollup\" and \"drilldown\" were coined by vendors of OLAP (OnLine Analytical Processing) systems that do real-time ETL from transaction data but the terms can be readily applied to any dimensional data warehouse.\n","\n","A **rollup** is the standard `GROUP BY` aggregation operation. The idea is that a whole stripe of data in the cube (i.e., a dimension) is \"rolled up\" like a carpet and then replaced with summary data. We can do this for several dimensions at a time to get summarized data of various purposes. \n","\n","A **drilldown** is the opposite of a rollup. Starting with aggregate data, a drilldown disaggregates it to a finer level of granularity. Behind the scenes, it is the same as a rollup, just with a lower level of aggregation. Ultimately, the lowest level of a drilldown is the fact table itself. "]},{"cell_type":"markdown","metadata":{"id":"DKpvT0vaTrwH"},"source":["---\n","## **NBA LineupFacts Data Mart**\n","\n","A data mart is most typically generated from two data sources:\n","- A custom rollup extracted from the data warehouse\n","- Supplementary data and calculations provided by the analysts \n","\n","The `lineup_facts` dataset from Lesson 3 (see below) was created to support a research project on the effect of teamwork in basketball. It includes a fully denormalized table (facts w / dimension columns) `play_facts_all`, from which we can calculate various statistics about the usage and efficiencies of basketball lineups over the years. \n","\n","![](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L3_GCP_data_explorer.png)\n","\n","Before importing into Tableau in Lesson 9 we created a `segment_facts` table that rolls up the *play segments* to *lineup segments*. A lineup segment is a sequence of play segments where neither lineup (home or away) changes. For a game with 500+ play segments, there might be 25 or fewer lineup segments, with each segment lasting an average of about 2 minutes. \n","\n","However, even though the `segment_facts` table is 20x smaller than the original `play_facts_all` table, it is still too large to fit into MS Excel or similar desktop software. For that there is the `season_facts_all` table, which rolls the `segment_facts` table by lineup and year. It *just barely* fits into MS Excel, with just over 200K rows and 35 columns. Besides rolling up the original `play_facts_all` columns, the `season_facts_all` table includes a new column, `plus_minus_36m`, that is needed for the research questions that motivated the study. \n","\n","![Season Facts in Excel](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L10_Lineup_Season_Facts.png)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A0BMRLxqZwqZ"},"source":["---\n","## **PRO TIPS: How to work with longitudinal data**\n","\n","Longitudinal data, where the meaning of a given fact is determined relative to the ones before it, is inherently problematic. The complexity is subtle, practically invisible to untrained eyes, but it can nonetheless wreak havoc on even the best data model. \n","\n","The relational data model is designed to capture instantaneous snapshots of whatever is being tracked, adding one row at a time. \n","```sql\n","INSERT INTO table (columns)\n","  rows;\n","```\n","\n","Similarly, `SELECT` queries return data one row at a time. That makes it nice and symmetric. Rows in a rows out. \n","\n","Now for the subtle part. If we want to include a calculated column, say the difference between two other columns in the same row, then we'd do something like this:\n","```sql\n","SELECT column_b - column_a AS b_minus_a \n","FROM ... ;\n","```\n","That seems simple enough. We can also, of course, use `GROUP BY` to do totals of a given column if we like, but the basic idea is still the same.  \n","\n","However, what if we need to calculate differences between each row and *the one above it*? Then it gets more complicated. Not even `GROUP BY` can help us with that, unless we somehow make each row and the one above it a group, which seems highly inefficient at best. We'll instead have to dig deeper into our bag of tricks: \n","- Join a table to itself using some sort of lag logic on the keys:\n","```\n","SELECT t2.column_a - t1.column_a AS diff_a_lag_1\n","FROM table AS t1 JOIN table AS t2 ON (t1.id = t2.id + 1)\n","```\n","- Use Python or another programming language to do the work. pandas has the `shift()` method just for such a purpose. You merge one table with a shifted version of itself. If you go this route, of course, you will need SQL to read the input data and then write the calculations back to the database afterwards. That's a nontrivial solution to say the least.  \n","- Use a SQL **window function**, which operates vertically within a column instead of laterally within a row. \n","\n","For most use cases, window functions will be your best choice. \n","\n","Window functions are used in the `SELECT` and `ORDER BY` clauses to allow longitudinal aggregates to be included *without a `GROUP BY` clause*. Instead of the traditional grouping operation, the expression being calculated specifies a range of rows, anchored by the current row. The syntax is \n","```sql\n","expression OVER ( window_selector )\n","```\n","The `window_selector` specifies what rows to include and can use pretty sophisticated selection criteria like any of the following:\n","- X rows before and Y rows after the current row\n","- All rows before the current row\n","- All rows where column X is within some range of Z in the current row\n","- X rows before and Y rows after, sorted by the value of Z\n","\n","The window function partitions the rows (much like `GROUP BY`) based on the given criteria and then calculates the expression based on the current row. The expression is almost always an aggregation function like `SUM()` or `AVG()`. of which there are too many to list here. There are even special ones like [`STRING_AGG()`](https://cloud.google.com/bigquery/docs/reference/standard-sql/functions-and-operators#string_agg) that work with non-numeric data types. \n","\n","Here, for example, we are calculating the cumulative minutes for each NBA player lineup throughout a season of play.  \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aD5HZzTObU0b","executionInfo":{"status":"ok","timestamp":1615921627059,"user_tz":240,"elapsed":17700,"user":{"displayName":"Christopher Huntley","photoUrl":"","userId":"11069151036176747256"}},"outputId":"740856c2-1e4a-42b4-bf70-4d0bd26810de"},"source":["# load the bigquery magics extension\n","%load_ext google.cloud.bigquery\n","\n","from google.colab import auth\n","auth.authenticate_user()\n","print('Authenticated')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Authenticated\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":419},"id":"3dUga94Fbq0j","executionInfo":{"status":"ok","timestamp":1615921634878,"user_tz":240,"elapsed":2225,"user":{"displayName":"Christopher Huntley","photoUrl":"","userId":"11069151036176747256"}},"outputId":"900093d1-c5a3-4cd1-b60b-b21050da3b5d"},"source":["%%bigquery --project nba-lineup-facts\n","SELECT \n","    game_id, \n","    date,\n","    lineup, \n","    sum(play_length_mins) OVER (PARTITION BY lineup ORDER BY game_id ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW ) AS cum_mins\n","FROM `nba-lineup-facts.lineup_facts.game_facts_all` \n","WHERE year = 2020\n","ORDER BY lineup, date\n","LIMIT 100"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>game_id</th>\n","      <th>date</th>\n","      <th>lineup</th>\n","      <th>cum_mins</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>21900415</td>\n","      <td>2019-12-18</td>\n","      <td>[\"D'Angelo Russell\", 'Damion Lee', 'Draymond G...</td>\n","      <td>0.083333</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>21900665</td>\n","      <td>2020-01-22</td>\n","      <td>[\"D'Angelo Russell\", 'Damion Lee', 'Draymond G...</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>21900679</td>\n","      <td>2020-01-24</td>\n","      <td>[\"D'Angelo Russell\", 'Damion Lee', 'Draymond G...</td>\n","      <td>4.233333</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>21900757</td>\n","      <td>2020-02-05</td>\n","      <td>[\"D'Angelo Russell\", 'Damion Lee', 'Draymond G...</td>\n","      <td>13.450000</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>21900757</td>\n","      <td>2020-02-05</td>\n","      <td>[\"D'Angelo Russell\", 'Damion Lee', 'Draymond G...</td>\n","      <td>1.733333</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>21900602</td>\n","      <td>2020-01-14</td>\n","      <td>[\"D'Angelo Russell\", 'Draymond Green', 'Glenn ...</td>\n","      <td>2.666667</td>\n","    </tr>\n","    <tr>\n","      <th>96</th>\n","      <td>21900588</td>\n","      <td>2020-01-12</td>\n","      <td>[\"D'Angelo Russell\", 'Draymond Green', 'Glenn ...</td>\n","      <td>1.500000</td>\n","    </tr>\n","    <tr>\n","      <th>97</th>\n","      <td>21900602</td>\n","      <td>2020-01-14</td>\n","      <td>[\"D'Angelo Russell\", 'Draymond Green', 'Glenn ...</td>\n","      <td>2.183333</td>\n","    </tr>\n","    <tr>\n","      <th>98</th>\n","      <td>21900076</td>\n","      <td>2019-11-01</td>\n","      <td>[\"D'Angelo Russell\", 'Draymond Green', 'Glenn ...</td>\n","      <td>3.216667</td>\n","    </tr>\n","    <tr>\n","      <th>99</th>\n","      <td>21900016</td>\n","      <td>2019-10-24</td>\n","      <td>[\"D'Angelo Russell\", 'Draymond Green', 'Glenn ...</td>\n","      <td>3.833333</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 4 columns</p>\n","</div>"],"text/plain":["     game_id  ...   cum_mins\n","0   21900415  ...   0.083333\n","1   21900665  ...   0.700000\n","2   21900679  ...   4.233333\n","3   21900757  ...  13.450000\n","4   21900757  ...   1.733333\n","..       ...  ...        ...\n","95  21900602  ...   2.666667\n","96  21900588  ...   1.500000\n","97  21900602  ...   2.183333\n","98  21900076  ...   3.216667\n","99  21900016  ...   3.833333\n","\n","[100 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"ReBjmsK5dN_8"},"source":["The window specification is the part inside the second parentheses:\n","```sql \n","PARTITION BY lineup \n","ORDER BY game_id \n","ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW \n","```\n","- `PARTITION BY` works like a `GROUP BY` to split the rows into groups/partitions.\n","- `ORDER BY` does as expected, but always with respect to the partition.\n","- `ROWS` specifies the width of the window; in this case that means everything up to the current row in the current partition (lineup).\n","\n","There are lots of varieties to play with. For details read the docs for your database vendor. The SQL standard defines the basics like `PARTITION BY`, `ORDER BY`, and `ROWS` but there are some differences for more advanced use cases.  "]},{"cell_type":"markdown","metadata":{"id":"o_9fblcZcJMQ"},"source":["> **Heads Up**: Although Window functions were included in SQL:2003, most of the vendors only recently implemented the standard.  Check to see if your DBMS supports them before trying to use them.  "]},{"cell_type":"markdown","metadata":{"id":"mnkGOtS8UwQ-"},"source":["---\n","## **SQL AND BEYOND: Arrays, Structs, and Partitions**"]},{"cell_type":"markdown","metadata":{"id":"qVL7n_Rys_95"},"source":["---\n","## **Congratulations! You've made it to the end of Lesson 10.**\n","\n","Next week we will consider alternative data models that can improve flexibility and performance. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"wEkhlPF5A9w1"},"source":["## **On your way out ... Be sure to save your work**.\n","In Google Drive, drag this notebook file into your `BUAN6510` folder so you can find it next time."]}]}