{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L11_NoSQL_HighPerformance_DBMS.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"11MBPducEj0HgT0u-nrCE60AvHJQnPR3y","authorship_tag":"ABX9TyMkh8nQYlmTdMtTxsalHFEH"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5rHzwUorF5ab"},"source":["<img src=\"https://github.com/christopherhuntley/BUAN6510/blob/master/img/Dolan.png?raw=true\" width=\"180px\" align=\"right\">\n","\n","# **BUAN 6510**\n","# **Lesson 11: NoSQL and Performance Tradeoffs** \n","_For when a centralized row store just won't work_\n","\n","## **Learning Objectives**\n","### **Theory / Be able to explain ...**\n","- The four concerns that drive most database implementation decisions\n","- The three major kinds of NoSQL models and when they are most applicable\n","- How each NoSQL model relates to SQL technology; how is it different and how is it the same?\n","- The CAP theorem and its implications for distributed databases\n","- What problems columnar database technology solves vis-a-vis traditional row-oriented RDBMS\n","\n","### **Skills / Know how to ...**\n","- Represent sparse datasets as Key-Value pairs\n","- Handle JSON data in a relational database\n","\n","--------\n","## **LESSON 11 HIGHLIGHTS**"]},{"cell_type":"code","metadata":{"id":"I95WVwkCnTJF","colab":{"base_uri":"https://localhost:8080/","height":580},"cellView":"form","executionInfo":{"status":"ok","timestamp":1614963855819,"user_tz":300,"elapsed":303,"user":{"displayName":"Christopher Huntley","photoUrl":"","userId":"11069151036176747256"}},"outputId":"13601f57-dc12-40ea-eabf-f6e795a84d13"},"source":["#@title Run this cell if video does not appear\n","%%html\n","<div style=\"max-width:1000px\">\n","  <div style=\"position: relative;padding-bottom: 56.25%;height: 0;\">\n","    <iframe style=\"position: absolute;top: 0;left: 0;width: 100%;height: 100%;\" rel=\"0\" modestbranding=\"1\"  src=\"https://www.youtube.com/embed/uRoW2sojmsE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","  </div>\n","</div>"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"max-width:1000px\">\n","  <div style=\"position: relative;padding-bottom: 56.25%;height: 0;\">\n","    <iframe style=\"position: absolute;top: 0;left: 0;width: 100%;height: 100%;\" rel=\"0\" modestbranding=\"1\"  src=\"https://www.youtube.com/embed/uRoW2sojmsE\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","  </div>\n","</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"euT4WJUOymsJ"},"source":["## **BIG PICTURE: Physical Design Alternatives**\n","In this lesson we will explore various alternatives to the traditional relational DBMS. Some of the alternatives will drop SQL itself, which would seem like heresy. However, there are actually some important use cases where one might not want to use a SQL-based solution. Even when SQL is the right decision there are plenty of options to choose from that fit some use cases better than others. \n","\n","In Lesson 5 we discussed four basic design priorities:\n","- Minimizing Storage Space\n","- Maximizing Calculation Speed\n","- Maximizing Coherency\n","- Minimizing Data Corruption Risk\n","\n","These tradeoffs between these priorities exist regardless of the technology used. That's why they are *design* tradeoffs. However, there is more to database systems than design. Sometimes we have to broaden our scope to consider other alternatives. \n","\n","Here are four considerations that underlie any database technology decision:\n","- **Flexibility / Developer Experience**  \n","  How easy is it for developers to learn and use the technology? It may make sense to use technology that is closer to what your programmers use everyday. \n","- **Scalability / Performance Speed and Cost**  \n","  There is a natural tradeoff between speed and cost. Technology that works at Big Data Scale makes that tradeoff explicit. How much does each GB of storage cost? How about each GB/sec of query throughput? If we are willing to wait a little longer for each query can we save some money? \n","- **Consistency / Timeliness**  \n","  Data is worthless if it is not available when you need it. If we don't need data to be instantaneously available, how long can we wait? If we need the data right now, then how tolerant are we of anomalies and other imperfections? If we use data replication to speed up access, do we need all copies to be 100% consistent? \n","- **Technical Maturity / Technical Debt**  \n","  Database technology is always evolving, with new solutions coming out all the time. The newest technology may score well on the above considerations but also may come with bugs and other problems that need to be worked out. Meanwhile, older technology may be rock solid but also may limit your choices going forward. There is always the risk of being stuck with obsolete technology while your competition is beating you with something newer. \n","\n","We will start with the first two considerations, developer experience and performance, which are generally used as the rationale for NoSQL technologies that don't (necessarily) adhere to the relational database model. Then we will follow up with strategies that can further improve the speed and cost performance for any technology, providing you are able to make the right consistency and maturity tradeoffs.\n"]},{"cell_type":"markdown","metadata":{"id":"yUNZeFcYCU9A"},"source":["---\n","## **NoSQL Databases**\n","The term \"NoSQL\" was first coined in the late 1990s and gained popularity among application programmers about a decade later. It refers to databases that do not rely on the relational model. That does not mean that SQL (or some close approximation) isn't used, but rather, that NoSQL systems extend beyond the traditional relational model. \n","\n","For this reason some interpret NoSQL as \"Not only SQL\" rather than exclusively no use of SQL at all. In fact, each of the NoSQL technologies surveyed here *could* in fact be implemented in SQL, and we will try to use relational models to explain how each relates to SQL and how it extends beyond it. \n","\n","Why would we even need to go beyond SQL? \n","- **Developer Experience (DX):** As we have discussed before, there is a natural impedance mismatch between a declarative language like SQL and a more imperative application development language like Python, Java, or JavaScript. NoSQL technologies remove much of the discomfort some programmers feel when using SQL.\n","- **Performance:** Selectively relaxing the rules of the relational model can sometimes bring speed and cost benefits that outweigh the integrity protections of the relational model. \n","\n","For each of the models below we will discuss how it differs from the standard relational model, potential DX or performance benefits, and the most common usage scenarios. \n","\n","### **Key-Value Stores**\n","Key-Value (KV) stores are most useful when a table is very sparse, like this section of the NBA PlayLog data set. \n","\n","![Sparse Table](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L11_Sparse_Table.png)\n","\n","If most values in a table are blank, then why waste time and effort recording them in rows and columns. Instead, just store the data you actually have, tagged to suit however you may need to retrieve it. \n","\n","In relational terms, a KV store is just a single two column table \n","`kv_store(`**`key`**`, value)`\n","\n","where\n","- **`key`** is a unique index\n","- `value` is a datum to be stored\n","\n","Usually the data type of the `value` is either encoded in the data itself (e.g., like i2'15' or s5'Steve`), encoded in the key, or assumed to be text. \n","\n","> **Heads Up:** We have already seen an example in Lesson 5. The Entity-Attribute-Value model can, with the right modeling conventions, be seen as a kind of KV store. The key would be a composite of the entity and attribute. \n","\n","You may be wondering how we can replace a two-dimensional table with rows and columns with a one dimensional KV store. We do it exactly like we would with composite indexes in the relational model, with the row and column encoded in the key. It's all in the patterns we use when constructing the keys.\n","\n","For example, the following could be used to record the `assist`,`away`, etc. columns as key-value pairs:\n","\n","| **Key** | **Value**|\n","| --- | ---|\n","| 2:away | Derrick Favors |\n","| 2:home | Marc Gasol |\n","| 18:opponent | Kyle Lowry |\n","| 20:num | 1 |\n","| 21:num | 2 |\n","\n","KV Stores first are commonly used for creating data caches for the web. Here, for example, is the data that Colab is keeping about *this page* (somewhat redacted) while I have it open in my web browser : \n","![](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L11_Colab_Local_Storage.png)\n","\n","Similar caching technology is used by web servers to minimize reads from the disk storage. If the same CSS is used on every page then why read it from disk every time? The server doesn't. Instead it serves the CSS from a highly-optimized, in-memory KV cache. \n","\n","#### **Wide-Column KV Stores**\n","Wide-Column data stores extend the KV model to allow data storage as multiple columns:\n","- The keys are just like any other KV store.\n","- The columns are encoded like a SQL `STRUCT` (or JSON object), with each column having a name and a value. \n","\n","The columns for each key may vary, like this:\n","\n","| **Key** | **Value**|\n","| --- | ---|\n","| 2 | {away:Derrick Favors, home: Marc Gasol} |\n","| 18 | {opponent:Kyle Lowry} |\n","| 20 | {num:1} |\n","| 21 | {num:2} |\n","\n","The effect is to condense the already very space-efficient key-value model by eliminating overlapping keys. In this case we eliminated a row of data storage by sharing the key `2` for the `away` and the `home` columns. \n","\n","#### **Summary**\n","\n","- Pros\n","  - very fast storage and retrieval (when applicable)\n","  - compact storage \n","  - programmer-friendly\n","- Cons\n","  - schema by convention instead of rules\n","  - potential for schema drift as new key types proliferate\n","  - no FKs or analogous way to integrate data across keys\n","- When to use\n","  - with sparse or highly volatile data where the keys need to be flexible\n","  - as local data stores in application development\n","  - when caching data to speed up application performance\n","- Example Products\n","  - [Varnish](https://varnish-cache.org/)\n","  - Nginx\n","  - Squid\n","  - Memcached\n","  - redis\n","  - AWS DynamoDB\n","\n","### **Document Stores**\n","Document stores build on the KV model to construct arbitrarily complex (and data rich) databases of *semi-structured* data. Semi-structured data has a schema (so we can interpret and process it), just like any other data model, but not one we can know about in advance. Specifically, in a document store each document (roughly equivalent to a table row) can have its own schema, structuring the data however it likes. We can then only know a document schema *after* opening the document, whereas for the relational model all table schemas are known *before* doing any DML operation. \n","\n","The classic example of a document storage format is that used by Microsoft Word, which acts as a *container* for components (blocks of text, titles, images, tables, etc.). One can insert just about anything inside an MS Word file (even malware, unfortunately). MS Word will then compose and render the components as documents in real time so end users can read and edit the data contained inside. \n","\n","A more relevant example is JSON, which has become the de facto standard format for transmitting data over the web. Like with an MS Word document, a JSON object or list acts as a container, into which we can insert ... JSON objects and lists. The result is a tree structure, with objects and lists nested inside each other to some unspecified depth. \n","\n","Here, for example, is what a Colab notebook (this one, in fact) looks like when you open it in a text editor:\n","![Colab as JSON](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L11_Colab_As_JSON.png)\n","\n","Yes, the `ipynb` file format is really just highly structured JSON. Here's the same JSON in a nicer, pretty printed format:\n","![Prettified JSON](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L11_Pretty_JSON.png)\n","\n","A Colab Notebook is mostly a list of \"cells\" $-$ look for it in the screenshot $-$ where each cell has a `cell_type`, `metadata`, and `source`. The `source` is whatever we typed into the cell. \n","\n","We will come back to JSON in the **Pro Tips** section later in this lesson. \n","\n","#### **Summary**\n","- Pros\n","  - little or no impedance mismatch for programmers, especially when using JavaScript\n","  - very compact, especially for semi-structured data\n","- Cons\n","  - same as KV stores; complex queries are especially difficult\n","  - schema on read complicates app design and development; potentially buggy\n","- When to use\n","  - for local storage or web transmission of data\n","  - for complex hierarchically-structured data, where documents are composed of nested components\n","  - When storing \"objects\" in relational databases\n","- Example Products\n","  - CouchDB\n","  - mongoDB\n","  - AWS DynamoDB\n","  - Google Cloud Firestore\n","\n","### **Graph Databases**\n","A graph database organizes data into three kinds of structures:\n","- A set of **nodes** that represent entities within the domain\n","- A set of **edges** (or arcs) that connect nodes to represent relationships\n","- **Properties** that represent the attributes of each entity or relationship\n","\n","| ![Neo4J Screenshot](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L11_Neo4J_Screenshot_Annotated.png) |\n","|:---:|\n","| Original Image Source: [*Graph Databases, Linked Data, RDF, and the Semantic Web Wasteland*]( https://medium.com/@eisenzopf/graph-databases-linked-data-rdf-and-the-semantic-web-wasteland-69e9f4347a5b) |\n","\n","\n","On a superficial level, graph databases are a lot like relational databases, with nodes = entities, properties = attributes, and edges = relationships.  A graph database, however, is much more flexible in how it defines these things: \n","- Since we are not using relations, there is no need to separate the entities by type. If we like, **we can treat each node as a unique type, with its own distinct properties (kept as Key-Value pairs).** \n","- Whereas a relational database treats relationships as being between entity types, graph databases define them on specific node pairs. **Any pair of nodes can be connected by an edge, regardless of type.** \n","- While each node has a unique identifier, there is no concept of type-based foreign keys. Instead, **each edge is treated like an entity with a unique identifier.** The edge then has two implicit properties to reference the nodes it connects. \n","\n","With all that said, a graph database *can* be mapped into a very specific relational model where:\n","- There are three types of entities: nodes, edges, and properties.\n","- Edge entities are directional, with each having a start node and an end node.\n","- Properties are associated with nodes or edges as needed.\n","\n","So if the graph database is really just a special case of the relational model, then what's the point? For some applications a graph database can be much, much faster than a relational database. For example, consider the database behind the [Six Degrees of Kevin Bacon](https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon) game, where each working actor has a *Bacon Number*, BN, that represents the number of edges needed to connect them to Kevin Bacon. Kevin Bacon's BN is 0. Anyone who has appeared in a movie with him has a BN of 1. Anyone who has appeared with anyone whose BN=1 has a BN of 2, etc. The game is to guess an obscure actor's BN, with super bonus points for finding a movie actor *anywhere in the world* without a BN. \n","\n","Let all the movie credits in the world be stored in a single table called `credits`, with one row per actor and movie. To keep things as simple as possible we will assume that \n","- Each actor and each movie has a unique identifier (`actor_id` and `movie_id`).\n","- We already know the `actor_id` identifiers for Kevin Bacon (`kevin_bacon_id`) and the other actor in question (`other_actor_id`).\n","\n","Then to find all people with BN=1 we would join the credits table with itself one time:\n","\n","```sql\n","-- Check for BN=1\n","SELECT distinct c1.actor_id \n","FROM credits AS c0 \n","    JOIN credits AS c1 USING (movie_id)\n","WHERE c0.actor_id = kevin_bacon_id AND c1.actor_id = other_actor_id\n","```\n","(If we want to look for a particular actor, we can just include them as `c1.actor_id` in the `WHERE` clause. If there are no results then the actor's BN is greater than 1.) \n","\n","To find all the movies that BN=1 actors appeared in would we add in another join:\n","```sql\n","SELECT distinct c2.movie_id \n","FROM credits AS c0 \n","    JOIN credits AS c1 ON (c0.movie_id = c1.movie_id)\n","    JOIN credits AS c2 ON (c1.actor_id = c2.actor_id)\n","WHERE c0.actor_id = kevin_bacon_id\n","```\n","\n","We can then repeat the process with another join to get the BN=2 actors. To get the BN=3 actors we would add on two more joins to the chain. If we repeat the process out to BN=6 (the theoretical maximum for a working movie actor) then we would have 11 joins in the chain. \n","\n","```sql\n","SELECT distinct c11.actor_id \n","FROM credits AS c0 \n","    JOIN credits AS c1 ON (c0.movie_id = c1.movie_id) # BN=1\n","\n","    JOIN credits AS c2 ON (c1.actor_id = c2.actor_id)\n","    JOIN credits AS c3 ON (c2.movie_id = c3.movie_id) # BN=2\n","\n","    JOIN credits AS c4 ON (c3.actor_id = c4.actor_id)\n","    JOIN credits AS c5 ON (c4.movie_id = c5.movie_id) # BN=3\n","\n","    JOIN credits AS c6 ON (c5.actor_id = c6.actor_id)\n","    JOIN credits AS c7 ON (c6.movie_id = c7.movie_id) # BN=4\n","\n","    JOIN credits AS c8 ON (c7.actor_id = c8.actor_id)\n","    JOIN credits AS c9 ON (c8.movie_id = c9.movie_id) # BN=5\n","\n","    JOIN credits AS c10 ON (c9.actor_id = c10.actor_id)\n","    JOIN credits AS c11 ON (c10.movie_id = c11.movie_id) # BN=6\n","WHERE c0.actor_id = kevin_bacon_id AND c11.actor_id = other_actor_id\n","```\n","\n","Running up to six queries like this seems ugly but still plenty doable, until you realize that the `credits` table might have 50 million rows. **Even with supercomputer hardware the query could take virtually forever.** With a graph database and a well-tuned search algorithm, however, we can find the specific sequence of edges connecting any actor to any other in less than a second. We can make it *even faster* if we assume that one of the actors is Kevin Bacon. \n","\n","The secret is in the specificity of the graph database model. We don't need to explore all possible actor-to-movie-to-actor connections. Instead we only need to follow the connections along the shortest path, which can be found fairly quickly via dynamic programming. If we want it even faster we can try A* search with an \"explore big cast movies first\" strategy. \n","\n","The achilles heel of the graph database model is bulk computation. If we need to do the same repetitive operation millions of times, perhaps concurrently, then a relational or perhaps a document database is likely a better fit, depending on the operation in question. Graph databases are designed for representation and searching, and they does things very well, but not so much for statistical modeling or similar table-oriented applications. \n","\n","#### **Summary**\n","- Pros\n","  - flexible data model, with nodes, entities, and properties as first-class entity types\n","  - scales well for associative datasets with lots of relationships\n","- Cons\n","  - same as KV stores; complex queries are especially difficult \n","  - while searchable, lacks most of the computational power of the relational model\n","  - treating each edges and property as a separate entity may not be space efficient\n","- When to use\n","  - whenever the data is naturally represented as nodes, edges, and properties\n","  - for network-focused apps like customer relationship management, road navigation, certain search engine operations, and file systems\n","  - for semantic maps (e.g., in text analytics) where the relationships represent interactions, forces, hierarchies, etc.\n","- Example Products\n","  - Neo4j\n","  - RedisGraph\n","  - AWS Neptune\n","\n","  \n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"kINRkPuMdhMI"},"source":["---\n","## **Technology at Big Data Scale**\n","\n","While any of the logical data models (relational or NoSQL) can scale up for Big Data applications, they are only partial solutions. To complete the job, we also need technology that can scale up as well. \n","\n","Absent a miraculous, once-in-every-other generation technological innovation, scaling up to Big Data requires making performance tradeoffs that deviate from the classic ACID model (Lesson 8). Here we will consider two such approaches:\n","- Distributed databases trade off consistency for performance and cost\n","- Columnar databases trade off database write and update performance for read and calculation performance. \n","\n","We will consider each in some detail below. \n","\n","### **Distributed Databases**\n","\n","The classic approach to any huge, complex problem is *divide and conquer*:\n","break the problem into smaller, simpler problems that can be processed (solved) somewhat independently. Distributed databases apply this general approach to database systems, dividing the processing and storage of data among multiple (possibly georgraphically disperate) subsystems. \n","\n","The issue, of course, is the *independent processors* part of the design. How are they independent? Can be they be run concurrently, with only occasional coordination? Can the data itself be divided so that different chunks can be processed separately? If two processes are operating on the same data at once, how are conflicts resolved? \n","\n","The tradeoffs embodied by such questions are captured by Brewer's [CAP Theorem](https://en.wikipedia.org/wiki/CAP_theorem), which centers around three possible guarantees:\n","- **Consistency**: Every database read is either 100% up-to-date and accurate or throws an error\n","- **Availability**: Every query receives a response immediately (even if the result is not up-to-date)\n","- **Partition Tolerance**: Errors and delays in concurrent requests do not cause the system to fail \n","\n","The only design that satisfies all three conditions is a centralized DBMS, with \n","- **one data store** with infinite storage capacity (so no possibility of data redundancy)\n","- **one processor** that operates infinitely fast (so no waiting for results)\n","- **one active transaction** at a time (so no race conditions or potential lockouts)\n","\n","No infinitely capable database system like this has ever existed, of course. However, for many applications a large enough, fast enough, ACID-compliant relational database is more than sufficient. \n","\n","For the rest we will likely need to consider a distributed database design that relaxes one or more of our design assumptions. All of the available options use one or more of the following approaches:\n","\n","**Data replication and partitioning relaxes the one data store condition.** \n","- With replication, the same data is stored in multiple data stores. It is then the database system's job to enforce consistency *eventually* can through a behind-the-scenes synchronization process. \n","- With partitioning the data is split into several parts (partitions) that are stored in separate data stores. There is no data duplication (and thus no consistency issues) but there may be latency as the data is assembled from multiple partitions.\n","\n","**Multiprocessing relaxes the one processor condition.** With multiprocessing, there are multiple processing *nodes* interacting over a network. By allowing multiple processors, we can distribute work to be closer to where it is needed. The tradeoff is then that the network itself can cause delays, cascading errors, and deadlock conditions. \n","\n","**Multitasking relaxes the one active transaction condition.** By splitting time between multiple tasks, a single centralized processor can simulate multiple processors without the internal network delays. However, external network delays then become more likely, with each user possibly being very distant from the central processor. Also, if there are enough transactions active at the same time, then the processor can become overloaded, causing each transaction to slow to a crawl, as dictated by Little's Law.\n","\n","As one can imagine, there exists a wide variety of options between a totally centralized, one transaction at a time DBMS and a fully distributed system with multiple data stores, multiple processors, and concurrent transactions.  \n","\n","An excellent example of the latter is [Cockroach DB](https://www.cockroachlabs.com/product/), which uses a data replication, synchronous writes, and distributed consensus logic to ensure ACID compliance over a network. In some cases it may not be the fastest solution $-$ ACID compliance comes at a cost $-$ but with each additional node the system gets faster without losing consistency. \n","\n","In the *SQL and Beyond* section that closes this lesson we will consider Git, the source code management system used by millions of programmers around. The system operates as a fully-distributed database, with each programmer working on their own \"local\" copies of the source code files that are kept in sync with \"remote\" copies being worked on by others.   \n","\n","### **Row-stores vs Column-stores**\n","\n","RUM"]},{"cell_type":"markdown","metadata":{"id":"A0BMRLxqZwqZ"},"source":["---\n","## **PRO TIPS: How to Work with JSON Data in SQL**\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mnkGOtS8UwQ-"},"source":["---\n","## **SQL AND BEYOND: Git as a Distributed DBMS**"]},{"cell_type":"markdown","metadata":{"id":"qVL7n_Rys_95"},"source":["---\n","## **Congratulations! You've made it to the end of Lesson 11.**\n","\n","There is no Lesson 12. So maybe celebrate with your beverage of choice.  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"wEkhlPF5A9w1"},"source":["## **On your way out ... Be sure to save your work**.\n","In Google Drive, drag this notebook file into your `BUAN6510` folder so you can find it next time."]}]}