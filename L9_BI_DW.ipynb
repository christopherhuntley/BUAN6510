{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L9_BI_DW.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"11MBPducEj0HgT0u-nrCE60AvHJQnPR3y","authorship_tag":"ABX9TyMAix/8gfHrJEO66vRTQLLW"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"5rHzwUorF5ab"},"source":["<img src=\"https://github.com/christopherhuntley/BUAN6510/blob/master/img/Dolan.png?raw=true\" width=\"180px\" align=\"right\">\n","\n","# **BUAN 6510**\n","# **Lesson 9: Business Intelligence and Data Warehousing** \n","_Data Driven Analytics_\n","\n","## **Learning Objectives**\n","### **Theory / Be able to explain ...**\n","- \n","\n","### **Skills / Know how to ...**\n","- \n","\n","--------\n","## **LESSON 9 HIGHLIGHTS**"]},{"cell_type":"code","metadata":{"id":"I95WVwkCnTJF","colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"status":"ok","timestamp":1613839773367,"user_tz":300,"elapsed":287,"user":{"displayName":"Christopher Huntley","photoUrl":"","userId":"11069151036176747256"}},"outputId":"ac633a8b-1bb7-4da3-8607-99dd4aeb8fee"},"source":["#@title Run this cell if video does not appear\n","%%html\n","<div style=\"max-width:1000px\">\n","  <div style=\"position: relative;padding-bottom: 56.25%;height: 0;\">\n","    <iframe style=\"position: absolute;top: 0;left: 0;width: 100%;height: 100%;\" rel=\"0\" modestbranding=\"1\"  src=\"https://www.youtube.com/embed/6UsegPkeJKw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","  </div>\n","</div>"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div style=\"max-width:1000px\">\n","  <div style=\"position: relative;padding-bottom: 56.25%;height: 0;\">\n","    <iframe style=\"position: absolute;top: 0;left: 0;width: 100%;height: 100%;\" rel=\"0\" modestbranding=\"1\"  src=\"https://www.youtube.com/embed/6UsegPkeJKw\" frameborder=\"0\" allow=\"accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n","  </div>\n","</div>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"euT4WJUOymsJ"},"source":["## **BIG PICTURE: Business Intelligence is the tip of a long spear**\n","Business intelligence (BI) is about generating actionable insights from data. It informs decision making at all levels of a firm. Given what we know now, how did it get that way and what can we do to get the results we desire? In other words, how can we use data to make us *smarter*? \n","\n","It is important to draw a distinction between BI and the broader world of Business Analytics. BI is **descriptive** (about things that are happening or have already happened) and to some degree **prescriptive** (what can be done now) but **very rarely predictive**. Where it does employ predictions, the models are more likely to be developed and tuned by data scientists with highly specialized training in machine learning and computer science.  \n","\n","An *apropos* analogy is the difference between accounting and finance. Accounting models are about accurately capturing the past and present (so it can be distilled into financial reports), while financial models are about the future (so we can make financial decisions that affect the future). While it is certainly possible for an accountant to know a lot about finance and a financial analyst to know a lot of accounting, they are nonetheless different professions. \n","\n","It's hard not to chuckle when people lump business intelligence in with database systems. Data visualization, probably *the* key feature for most BI apps, is in the presentation tier, not the data tier. Given access to lots of preprocessed data, with all the anomalies smoothed out, a BI dashboard allows us to see whatever stories the data can tell. That's very important $-$ literally why we collect, clean, and protect the data in the first place $-$ but about as much like a database app as whiffleball is to baseball. They really are different things, with vastly different skillsets.\n","\n","![BI Pipelines](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L9_Data_Pipelines.png)\n","\n","The process of collecting, curating, and repackaging data for consumption by business analysts is called a **pipeline**. Starting with raw data from various sources $-$ more on that in a moment $-$ the process flows through several layers of processing to distill the data down to just what the business analyst needs. While much of this \"sausage making\" process is designed and managed by data engineers, it is (or should be) with the guidance and insights of data analysts. After all, who else really knows what the end product is supposed to be? \n","\n","In this lesson we will focus on data pipelines, starting with the general challenges and then highlighting the functional and technical differences between the layers.    "]},{"cell_type":"markdown","metadata":{"id":"NekcmKiCsxQm"},"source":["---\n","## **The Seven *V*s of Big Data Pipelines**\n","\n","It has long been tradition to characterize the unique challenges presented by big data with a laundry list of adjectives, all starting with the letter *V*. As we have gotten more familiar with the pitfalls of massive datasets, the  list has [expanded to seven](https://towardsdatascience.com/modern-unified-data-architecture-38182304afcc), each motivated by plenty of war stories:\n","\n","- **Variety**. Data can come from various disconnected sources, each of which makes its own assumptions about the domain. Even within a given domain, assumptions may change over time. So, what was perfectly valid data a decade ago is not necessarily useful today. \n","- **Velocity**. Modern online systems are really, really good at collecting data. It is like the data tier is trying to \"drink from a firehose\" as it sorts through it all in real-time. Anything that is not stored is lost, and not everything can be stored. We have to make hard decisions sometimes. \n","- **Volume**. The shear size of the datasets can be difficult to deal with. We saw that when trying to load the PlayLog database in Lesson 8. The dataset was large enough that only part of the data could be loaded at a time. Otherwise, we risked completely filling up the server. In fact, a server reboot was needed to finish the job. \n","- **Visbility**. There are always going to be new uses for a given dataset. Each time we try to support a new app or new model we may need a new interface, with data formatted a certain way, perhaps with data that has never been shared before. That opens up all sorts of potential bugs and other pitfalls. \n","- **Veracity**. Data can lie. No matter how much we can try to validate every fact, some bugs are going to remain. These kinds of bugs are unknowable and won't surface until uncovered by an application that tries to use the data. \n","- **Vulnerability**. As businesses become more and more dependent on data, the data itself becomes more and more enticing for cyber criminals. Even if they dont hold the data hostage, they may steal secrets that are best left hidden. \n","- **Value**. It is tempting to just dump everything you have into an over-engineered uber data warehouse. However, needs will inevitably change, requiring continual redesign. Maintenance is then a continual process, with each added design feature a potential liability. If a feature does not provide value, then kill it before it becomes a problem instead of a solution. \n","\n","While any given dataset may be subject to any of these issues, the risks go up exponentially with more data, compounding with each row of data. In other words ...\n","\n","**If it's big data, assume it's pig ugly and expect to spend 80% of your time applying lots of lipstick.**\n","\n","\n"," \n"]},{"cell_type":"markdown","metadata":{"id":"yUNZeFcYCU9A"},"source":["## **Lakes, Warehouses, and Marts**\n","\n","An enterprise-scale pipeline starts in the **data lake** layer, progresses through to a **data warehouse** layer, and then ends with a **data mart** layer. It is possible that a given company may combine or even skip one of these layers, of course. \n","\n","Many of the most common architectural choices are laid out in the following table, follow . \n","\n","|  | Data Lake | Data Warehouse | Data Mart |\n","|---|---|---|---|\n","| **Objective** | Retain Everything | Curate & Integrate Content | Package for Use |\n","| **User Access** | Read-Write | Read-only | Read-Write (views and extracts) |\n","| **Structured Data**| SQL | SQL, OLAP Cubes | SQL, Spreadhseets |\n","| **Semi-Structured Data** | JSON, Docs, NoSQL, APIs| SQL (with Extensions), Object-Relational DBs | SQL (with Extensions), NoSQL, Spreadsheets |\n","| **Unstructured Data** | Docs, Text Files, Streams | N/A | NoSQL, Reports |\n","| **Time Scale** | Now/Online | Historical, Online | Historical |\n","| **Storage Requirements** | Petabytes/Terabytes | Terabytes/Gigabytes | Gigabytes/Megabytes |\n","| **Storage Strategy** | Files + Row Stores | Column Stores | Documents / Various |\n","| **Example Tech** |  MySQL, PostreSQL, AWS DynamoDB ) | GCP BigQuery, AWS Dynamo, etc. | Google Sheets, DropBox Files, etc.|\n","\n","### **Functional Differences**\n","\n","A **data lake** is a repository for just about any kind of data. A conscious effort is made to retain the data in its original state, including all the bugs and other errors. \n","\n","A **data warehouse** is intended as the one true source for all data. It draws from the data lake but then uses business logic to clean it of errors, enrich it with summary facts, and integrate it into a coherent whole. The cleaning and integration are performed as part of ETL process that loads data into the warehouse. It is important to note that all data enters the data warehouse through managed the ETL processes. To everything else, access to the data warehouse is strictly read-only.\n","\n","**Data marts** are generated (extracted) from the data warehouse. Since the warehouse ensures that the data is clean and consistent, there is no need to keep data in a normalized form. In fact, it is usually best to denormalize the data as much as possible (i.e., one table with lots of possibly redundant columns) so as to avoid complex query logic. We can safely do this because the data is never fed back into the data warehouse. The extraction processes that feed the data marts are read-only users just like everybody else.  \n","\n","### **Data Structure and Format**\n","How the data is stored and processed depends in part on the degree of structure.\n","\n","**Structured data** is organized into datasets with a fixed data data model (*schema*). Ideally, the data comes from a well-designed relational database so that we can at least assume that it free of anomalies. If a change is made to the schema then the data is restructured to match. Thus, we always know what the schema is *before writing new data* to a dataset.  \n","\n","In general, structured data is best kept in a relational database, though there are sometimes reasons to consider alternatives. For example, graph databases are excellent at storing geo-spatial data. It is still highly structured, except the schema is not relational. \n","\n","**Semi-structured data** has metadata and perhaps a schema but there is no attempt to retain consistency over time. In other words, each datum may have its own schema, which *might not be known until the data is read*. This of course can cause some data integration and retrieval/search issues. \n","\n","The classic example of a semistructured data format is JSON, which organizes data into hierarchies (trees) of indeterminate depth. It *is* certainly possible to have a consistent structure in JSON but it is not guaranteed.\n","\n","**Unstructured data** does not have any schema at all. Raw text from a social media stream, for example, can be about anything. Similarly, photos that have not been tagged for content are just collections of pixels and lines. Data retrieval then becomes a matter of luck and intuition rather than a repeatable process. \n","\n","### **Performance Requirements and Technologies**\n","At Big Data scale raw performance matters, and how to best deliver that performance depends on where we are in the pipeline. \n","\n","In a data lake the emphasis is capturing and storing (writing) data in close to real time. That means any storage strategy needs to accept *serialized* data in the order it is collected. Files are transmitted in serialized anyway, so they can be stored as is. Relational data (i.e., in a relational database) comes in one transaction at a time, adding rows with each transaction.  MySQL, for example, is tuned to operate this way. It writes (and reads) individual rows of data really quickly. \n","\n","In a data warehouse the emphasis is on making queries run as fast as possible. Since most queries only use a few columns at a time, the best way to [structure the data is in columns](https://en.wikipedia.org/wiki/Column-oriented_DBMS). BigQuery, for example, uses this strategy to good effect, sometimes offering orders of magnitude speedup over MySQL for the same `SELECT` query. (Python programmers should also note that *pandas* also uses a column-oriented strategy: a DataFrame is equivalent to a dictionary of lists, one list per column.)\n","\n","In a data mart the best strategy depends entirely on the applications. In many cases it might be best to avoid using a database at all, making data available as CSV files, spreadsheets, or other document formats. In any case, the datasets (or databases) themselves are rarely large enough to make performance an issue.\n"]},{"cell_type":"markdown","metadata":{"id":"DKpvT0vaTrwH"},"source":["---\n","## **NBA PlayFacts Data Warehouse**\n","### **Designed for Analytics**\n","Any data warehouse is only as useful as the questions it answers. Whenever possible it should \n","- make calculating aggregate statistics as easy as possible using simple sums, averages, etc.\n","- allow the data to be grouped (labelled) in various ways that make sense to analysts\n","- allow statistics to be disaggretated to identify the base-level source data\n","- use keys and other indexes that are **idempotent** (i.e., time invariant) so that a report from years ago can be rerun today without a major redesign\n","\n","These requirements lead most naturally to a star schema design where:\n","- there is a central **fact table** with possibly many columns precomputed aggregable stats (facts) and dimensional labels (foreign keys) that can used to describe and categorize the facts\n","- surrounding the fact table are **dimension tables** that provide the labels and possibly more descriptive detail\n","\n","We call such a database a Dimensional Data Warehouse, which we will go into in more detail in Lesson 10. \n","\n","The ERD for the NBA PlayFacts warehouse is shown below:\n","![](https://github.com/christopherhuntley/BUAN6510/raw/master/img/L9_NBA_PlayFacts_ERD.png)\n","  \n","\n","### **ETL Process**\n","### **Extraction into Data Marts**\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"A0BMRLxqZwqZ"},"source":["---\n","## **PRO TIPS: How to Bulletproof Your ETL Process**\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"mnkGOtS8UwQ-"},"source":["---\n","## **SQL AND BEYOND: Tableau + BigQuery**"]},{"cell_type":"markdown","metadata":{"id":"qVL7n_Rys_95"},"source":["---\n","## **Congratulations! You've made it to the end of Lesson 9.**\n","\n","Next time we will focus more on data architecture, with a moderately deep dive into star schema models.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wEkhlPF5A9w1"},"source":["## **On your way out ... Be sure to save your work**.\n","In Google Drive, drag this notebook file into your `BUAN6510` folder so you can find it next time."]}]}